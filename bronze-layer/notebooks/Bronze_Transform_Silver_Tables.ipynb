{"cells":[{"cell_type":"code","source":["from pyspark.sql.functions import col, to_timestamp, lower, year, month, round\n","from pyspark.sql import DataFrame\n","\n","# =============================================================================\n","# 1. DEFINE TRANSFORMATION FUNCTIONS\n","# =============================================================================\n","\n","def transform_orders(df: DataFrame) -> DataFrame:\n","    \"\"\"Cleans and transforms the orders data.\"\"\"\n","    return df.withColumn(\"order_purchase_timestamp\", to_timestamp(col(\"order_purchase_timestamp\"))) \\\n","             .withColumn(\"order_approved_at\", to_timestamp(col(\"order_approved_at\"))) \\\n","             .withColumn(\"order_delivered_carrier_date\", to_timestamp(col(\"order_delivered_carrier_date\"))) \\\n","             .withColumn(\"order_delivered_customer_date\", to_timestamp(col(\"order_delivered_customer_date\"))) \\\n","             .withColumn(\"order_estimated_delivery_date\", to_timestamp(col(\"order_estimated_delivery_date\"))) \\\n","             .withColumn(\"order_status\", lower(col(\"order_status\"))) \\\n","             .withColumn(\"purchase_year\", year(col(\"order_purchase_timestamp\"))) \\\n","             .withColumn(\"purchase_month\", month(col(\"order_purchase_timestamp\"))) \\\n","             .na.drop(subset=[\"order_id\", \"customer_id\", \"order_purchase_timestamp\"])\n","\n","def transform_order_items(df: DataFrame) -> DataFrame:\n","    \"\"\"Cleans and transforms order items data.\"\"\"\n","    return df.withColumn(\"shipping_limit_date\", to_timestamp(col(\"shipping_limit_date\"))) \\\n","             .withColumn(\"price\", round(col(\"price\").cast(\"float\"), 2)) \\\n","             .withColumn(\"freight_value\", round(col(\"freight_value\").cast(\"float\"), 2)) \\\n","             .withColumn(\"order_item_id\", col(\"order_item_id\").cast(\"integer\"))\n","\n","def transform_payments(df: DataFrame) -> DataFrame:\n","    \"\"\"Cleans and transforms payments data.\"\"\"\n","    return df.withColumn(\"payment_sequential\", col(\"payment_sequential\").cast(\"integer\")) \\\n","             .withColumn(\"payment_installments\", col(\"payment_installments\").cast(\"integer\")) \\\n","             .withColumn(\"payment_value\", round(col(\"payment_value\").cast(\"float\"), 2))\n","\n","def transform_reviews(df: DataFrame) -> DataFrame:\n","    \"\"\"Cleans and transforms reviews data.\"\"\"\n","    return df.withColumn(\"review_creation_date\", to_timestamp(col(\"review_creation_date\"))) \\\n","             .withColumn(\"review_answer_timestamp\", to_timestamp(col(\"review_answer_timestamp\"))) \\\n","             .withColumn(\"review_score\", col(\"review_score\").cast(\"integer\")) \\\n","             .na.drop(subset=[\"order_id\", \"review_score\"])\n","\n","def transform_products(df: DataFrame) -> DataFrame:\n","    \"\"\"Cleans and transforms products data.\"\"\"\n","    return df.withColumnRenamed(\"product_name_lenght\", \"product_name_length\") \\\n","             .withColumnRenamed(\"product_description_lenght\", \"product_description_length\")\n","\n","def passthrough_transform(df: DataFrame) -> DataFrame:\n","    \"\"\"A default function for data that needs no transformation.\"\"\"\n","    return df\n","\n","# =============================================================================\n","# 2. DETECT AVAILABLE PATHS\n","# =============================================================================\n","\n","print(\"üîç Detecting available paths...\")\n","\n","# Try different path patterns for Bronze layer (based on working config)\n","bronze_paths_to_try = [\n","    \"abfss://Alex_Olist@onelake.dfs.fabric.microsoft.com/LH_Bronze_AY.Lakehouse/Files/\",\n","    \"/LH_Bronze_AY.Lakehouse/Files/\",\n","    \"LH_Bronze_AY.Lakehouse/Files/\",\n","    \"Files/\",\n","    \"\"\n","]\n","\n","# Test read path\n","bronze_base_path = None\n","for path in bronze_paths_to_try:\n","    try:\n","        test_file = f\"{path}olist_orders_dataset.csv\"\n","        print(f\"Testing read path: {test_file}\")\n","        test_df = spark.read.format(\"csv\").option(\"header\", \"true\").load(test_file)\n","        if test_df.count() > 0:\n","            bronze_base_path = path\n","            print(f\"‚úÖ Found working Bronze path: {bronze_base_path}\")\n","            break\n","    except Exception as e:\n","        print(f\"‚ùå Failed: {test_file}\")\n","\n","if not bronze_base_path:\n","    print(\"‚ùå Could not find Bronze layer files.\")\n","    exit()\n","\n","# =============================================================================\n","# 3. CREATE CONFIGURATION\n","# =============================================================================\n","\n","job_config = {\n","    \"orders\":       {\"bronze\": \"olist_orders_dataset.csv\", \"table\": \"silver_orders\", \"func\": transform_orders},\n","    \"customers\":    {\"bronze\": \"olist_customers_dataset.csv\", \"table\": \"silver_customers\", \"func\": passthrough_transform},\n","    \"order_items\":  {\"bronze\": \"olist_order_items_dataset.csv\", \"table\": \"silver_order_items\", \"func\": transform_order_items},\n","    \"products\":     {\"bronze\": \"olist_products_dataset.csv\", \"table\": \"silver_products\", \"func\": transform_products},\n","    \"sellers\":      {\"bronze\": \"olist_sellers_dataset.csv\", \"table\": \"silver_sellers\", \"func\": passthrough_transform},\n","    \"payments\":     {\"bronze\": \"olist_order_payments_dataset.csv\", \"table\": \"silver_order_payments\", \"func\": transform_payments},\n","    \"reviews\":      {\"bronze\": \"olist_order_reviews_dataset.csv\", \"table\": \"silver_order_reviews\", \"func\": transform_reviews},\n","    \"geolocation\":  {\"bronze\": \"olist_geolocation_dataset.csv\", \"table\": \"silver_geolocation\", \"func\": passthrough_transform},\n","    \"translations\": {\"bronze\": \"product_category_name_translation.csv\", \"table\": \"silver_product_translations\", \"func\": passthrough_transform}\n","}\n","\n","# =============================================================================\n","# 4. RUN THE TRANSFORMATION JOB - CREATE TABLES\n","# =============================================================================\n","\n","print(f\"\\nüöÄ Starting table transformation job...\")\n","print(f\"üìÅ Reading from: {bronze_base_path}\")\n","print(f\"üèóÔ∏è  Creating tables in: LH_Silver_AY\")\n","\n","successful_transformations = 0\n","\n","for key, config in job_config.items():\n","    bronze_filename = config[\"bronze\"]\n","    table_name = config[\"table\"]\n","    transform_function = config[\"func\"]\n","    \n","    read_path = f\"{bronze_base_path}{bronze_filename}\"\n","    \n","    try:\n","        print(f\"\\nüìä Processing '{key}': {bronze_filename} -> {table_name}\")\n","        \n","        # Step 1: Read the CSV file\n","        bronze_df = spark.read \\\n","            .format(\"csv\") \\\n","            .option(\"header\", \"true\") \\\n","            .option(\"inferSchema\", \"true\") \\\n","            .option(\"multiline\", \"true\") \\\n","            .option(\"escape\", '\"') \\\n","            .load(read_path)\n","        \n","        row_count = bronze_df.count()\n","        if row_count == 0:\n","            print(f\"‚ö†Ô∏è  WARNING: '{key}' appears to be empty, skipping...\")\n","            continue\n","        \n","        print(f\"   üìà Loaded {row_count} rows\")\n","        \n","        # Step 2: Apply transformation\n","        print(f\"   üîÑ Applying transformation: {transform_function.__name__}\")\n","        transformed_df = transform_function(bronze_df)\n","        \n","        # Step 3: Clean column names (Fabric table friendly)\n","        clean_columns = []\n","        for c in transformed_df.columns:\n","            # Make column names Fabric table friendly\n","            clean_name = c.lower().replace(\" \", \"_\").replace(\"-\", \"_\").replace(\".\", \"_\")\n","            clean_columns.append(clean_name)\n","        \n","        final_df = transformed_df.toDF(*clean_columns)\n","        \n","        # Step 4: Create temporary view first\n","        temp_view_name = f\"temp_{table_name}\"\n","        final_df.createOrReplaceTempView(temp_view_name)\n","        \n","        # Step 5: Drop existing table if it exists\n","        try:\n","            spark.sql(f\"DROP TABLE IF EXISTS LH_Silver_AY.{table_name}\")\n","        except:\n","            pass  # Table might not exist yet\n","        \n","        # Step 6: Create table in LH_Silver_AY lakehouse\n","        print(f\"   üèóÔ∏è  Creating table: LH_Silver_AY.{table_name}\")\n","        \n","        create_table_sql = f\"\"\"\n","        CREATE TABLE LH_Silver_AY.{table_name}\n","        USING DELTA\n","        AS SELECT * FROM {temp_view_name}\n","        \"\"\"\n","        \n","        spark.sql(create_table_sql)\n","        \n","        # Step 7: Verify table creation\n","        result_count = spark.sql(f\"SELECT COUNT(*) as count FROM LH_Silver_AY.{table_name}\").collect()[0]['count']\n","        \n","        print(f\"   ‚úÖ Successfully created table '{table_name}' with {result_count} rows\")\n","        successful_transformations += 1\n","        \n","        # Clean up temp view\n","        spark.sql(f\"DROP VIEW IF EXISTS {temp_view_name}\")\n","        \n","    except Exception as e:\n","        print(f\"   ‚ùå ERROR processing '{key}': {str(e)}\")\n","        print(f\"      Read path: {read_path}\")\n","        # Try to clean up temp view if it exists\n","        try:\n","            spark.sql(f\"DROP VIEW IF EXISTS temp_{table_name}\")\n","        except:\n","            pass\n","        continue\n","\n","print(f\"\\nüéâ Table transformation job completed!\")\n","print(f\"üìà Successfully created {successful_transformations}/{len(job_config)} tables\")\n","\n","# =============================================================================\n","# 5. DISPLAY TABLE INFORMATION\n","# =============================================================================\n","\n","if successful_transformations > 0:\n","    print(f\"\\nüìã Created tables in LH_Silver_AY:\")\n","    \n","    for key, config in job_config.items():\n","        table_name = config[\"table\"]\n","        try:\n","            # Get table info\n","            table_info = spark.sql(f\"DESCRIBE TABLE LH_Silver_AY.{table_name}\")\n","            row_count = spark.sql(f\"SELECT COUNT(*) as count FROM LH_Silver_AY.{table_name}\").collect()[0]['count']\n","            \n","            print(f\"\\nüóÉÔ∏è  Table: {table_name}\")\n","            print(f\"   üìä Rows: {row_count}\")\n","            print(f\"   üìù Columns: {table_info.count()}\")\n","            \n","            # Show first few column names\n","            columns = [row['col_name'] for row in table_info.select('col_name').collect()[:5]]\n","            print(f\"   üè∑Ô∏è  Sample columns: {', '.join(columns)}\")\n","            \n","        except Exception as e:\n","            print(f\"   ‚ùå Could not get info for {table_name}: {str(e)}\")\n","\n","    print(f\"\\n‚ú® All tables are now available in LH_Silver_AY lakehouse!\")\n","    print(f\"üí° You can now query them using: SELECT * FROM LH_Silver_AY.table_name\")\n","\n","# =============================================================================\n","# 6. SAMPLE QUERIES\n","# =============================================================================\n","\n","print(f\"\\nüìÑ Sample queries to test your new tables:\")\n","print(f\"   ‚Ä¢ SELECT * FROM LH_Silver_AY.silver_orders LIMIT 10\")\n","print(f\"   ‚Ä¢ SELECT COUNT(*) FROM LH_Silver_AY.silver_customers\") \n","print(f\"   ‚Ä¢ SELECT * FROM LH_Silver_AY.silver_products WHERE product_category_name IS NOT NULL LIMIT 5\")\n","print(f\"   ‚Ä¢ SHOW TABLES IN LH_Silver_AY\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":19,"statement_ids":[19],"state":"finished","livy_statement_state":"available","session_id":"683d243f-7cc7-4b06-af7e-979352f7fb30","normalized_state":"finished","queued_time":"2025-09-14T07:51:29.6342074Z","session_start_time":null,"execution_start_time":"2025-09-14T07:51:29.636766Z","execution_finish_time":"2025-09-14T07:53:19.8904051Z","parent_msg_id":"45cc6ce3-9443-41a2-96b7-70a6f98bc274"},"text/plain":"StatementMeta(, 683d243f-7cc7-4b06-af7e-979352f7fb30, 19, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["üîç Detecting available paths...\nTesting read path: abfss://Alex_Olist@onelake.dfs.fabric.microsoft.com/LH_Bronze_AY.Lakehouse/Files/olist_orders_dataset.csv\n‚úÖ Found working Bronze path: abfss://Alex_Olist@onelake.dfs.fabric.microsoft.com/LH_Bronze_AY.Lakehouse/Files/\n\nüöÄ Starting table transformation job...\nüìÅ Reading from: abfss://Alex_Olist@onelake.dfs.fabric.microsoft.com/LH_Bronze_AY.Lakehouse/Files/\nüèóÔ∏è  Creating tables in: LH_Silver_AY\n\nüìä Processing 'orders': olist_orders_dataset.csv -> silver_orders\n   üìà Loaded 99441 rows\n   üîÑ Applying transformation: transform_orders\n   üèóÔ∏è  Creating table: LH_Silver_AY.silver_orders\n   ‚úÖ Successfully created table 'silver_orders' with 99441 rows\n\nüìä Processing 'customers': olist_customers_dataset.csv -> silver_customers\n   üìà Loaded 99441 rows\n   üîÑ Applying transformation: passthrough_transform\n   üèóÔ∏è  Creating table: LH_Silver_AY.silver_customers\n   ‚úÖ Successfully created table 'silver_customers' with 99441 rows\n\nüìä Processing 'order_items': olist_order_items_dataset.csv -> silver_order_items\n   üìà Loaded 112650 rows\n   üîÑ Applying transformation: transform_order_items\n   üèóÔ∏è  Creating table: LH_Silver_AY.silver_order_items\n   ‚úÖ Successfully created table 'silver_order_items' with 112650 rows\n\nüìä Processing 'products': olist_products_dataset.csv -> silver_products\n   üìà Loaded 32951 rows\n   üîÑ Applying transformation: transform_products\n   üèóÔ∏è  Creating table: LH_Silver_AY.silver_products\n   ‚úÖ Successfully created table 'silver_products' with 32951 rows\n\nüìä Processing 'sellers': olist_sellers_dataset.csv -> silver_sellers\n   üìà Loaded 3095 rows\n   üîÑ Applying transformation: passthrough_transform\n   üèóÔ∏è  Creating table: LH_Silver_AY.silver_sellers\n   ‚úÖ Successfully created table 'silver_sellers' with 3095 rows\n\nüìä Processing 'payments': olist_order_payments_dataset.csv -> silver_order_payments\n   üìà Loaded 103886 rows\n   üîÑ Applying transformation: transform_payments\n   üèóÔ∏è  Creating table: LH_Silver_AY.silver_order_payments\n   ‚úÖ Successfully created table 'silver_order_payments' with 103886 rows\n\nüìä Processing 'reviews': olist_order_reviews_dataset.csv -> silver_order_reviews\n   üìà Loaded 99224 rows\n   üîÑ Applying transformation: transform_reviews\n   üèóÔ∏è  Creating table: LH_Silver_AY.silver_order_reviews\n   ‚úÖ Successfully created table 'silver_order_reviews' with 99224 rows\n\nüìä Processing 'geolocation': olist_geolocation_dataset.csv -> silver_geolocation\n   üìà Loaded 1000163 rows\n   üîÑ Applying transformation: passthrough_transform\n   üèóÔ∏è  Creating table: LH_Silver_AY.silver_geolocation\n   ‚úÖ Successfully created table 'silver_geolocation' with 1000163 rows\n\nüìä Processing 'translations': product_category_name_translation.csv -> silver_product_translations\n   üìà Loaded 71 rows\n   üîÑ Applying transformation: passthrough_transform\n   üèóÔ∏è  Creating table: LH_Silver_AY.silver_product_translations\n   ‚úÖ Successfully created table 'silver_product_translations' with 71 rows\n\nüéâ Table transformation job completed!\nüìà Successfully created 9/9 tables\n\nüìã Created tables in LH_Silver_AY:\n\nüóÉÔ∏è  Table: silver_orders\n   üìä Rows: 99441\n   üìù Columns: 10\n   üè∑Ô∏è  Sample columns: order_id, customer_id, order_status, order_purchase_timestamp, order_approved_at\n\nüóÉÔ∏è  Table: silver_customers\n   üìä Rows: 99441\n   üìù Columns: 5\n   üè∑Ô∏è  Sample columns: customer_id, customer_unique_id, customer_zip_code_prefix, customer_city, customer_state\n\nüóÉÔ∏è  Table: silver_order_items\n   üìä Rows: 112650\n   üìù Columns: 7\n   üè∑Ô∏è  Sample columns: order_id, order_item_id, product_id, seller_id, shipping_limit_date\n\nüóÉÔ∏è  Table: silver_products\n   üìä Rows: 32951\n   üìù Columns: 9\n   üè∑Ô∏è  Sample columns: product_id, product_category_name, product_name_length, product_description_length, product_photos_qty\n\nüóÉÔ∏è  Table: silver_sellers\n   üìä Rows: 3095\n   üìù Columns: 4\n   üè∑Ô∏è  Sample columns: seller_id, seller_zip_code_prefix, seller_city, seller_state\n\nüóÉÔ∏è  Table: silver_order_payments\n   üìä Rows: 103886\n   üìù Columns: 5\n   üè∑Ô∏è  Sample columns: order_id, payment_sequential, payment_type, payment_installments, payment_value\n\nüóÉÔ∏è  Table: silver_order_reviews\n   üìä Rows: 99224\n   üìù Columns: 7\n   üè∑Ô∏è  Sample columns: review_id, order_id, review_score, review_comment_title, review_comment_message\n\nüóÉÔ∏è  Table: silver_geolocation\n   üìä Rows: 1000163\n   üìù Columns: 5\n   üè∑Ô∏è  Sample columns: geolocation_zip_code_prefix, geolocation_lat, geolocation_lng, geolocation_city, geolocation_state\n\nüóÉÔ∏è  Table: silver_product_translations\n   üìä Rows: 71\n   üìù Columns: 2\n   üè∑Ô∏è  Sample columns: product_category_name, product_category_name_english\n\n‚ú® All tables are now available in LH_Silver_AY lakehouse!\nüí° You can now query them using: SELECT * FROM LH_Silver_AY.table_name\n\nüìÑ Sample queries to test your new tables:\n   ‚Ä¢ SELECT * FROM LH_Silver_AY.silver_orders LIMIT 10\n   ‚Ä¢ SELECT COUNT(*) FROM LH_Silver_AY.silver_customers\n   ‚Ä¢ SELECT * FROM LH_Silver_AY.silver_products WHERE product_category_name IS NOT NULL LIMIT 5\n   ‚Ä¢ SHOW TABLES IN LH_Silver_AY\n"]}],"execution_count":17,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"d70afdf5-18ee-4655-8084-2f46aa9d01cb"},{"cell_type":"code","source":["from pyspark.sql.functions import col, to_timestamp, lower, year, month, round\n","from pyspark.sql import DataFrame\n","\n","# =============================================================================\n","# 1. DEFINE TRANSFORMATION FUNCTIONS\n","# =============================================================================\n","\n","def transform_orders(df: DataFrame) -> DataFrame:\n","    \"\"\"Cleans and transforms the orders data.\"\"\"\n","    return df.withColumn(\"order_purchase_timestamp\", to_timestamp(col(\"order_purchase_timestamp\"))) \\\n","             .withColumn(\"order_approved_at\", to_timestamp(col(\"order_approved_at\"))) \\\n","             .withColumn(\"order_delivered_carrier_date\", to_timestamp(col(\"order_delivered_carrier_date\"))) \\\n","             .withColumn(\"order_delivered_customer_date\", to_timestamp(col(\"order_delivered_customer_date\"))) \\\n","             .withColumn(\"order_estimated_delivery_date\", to_timestamp(col(\"order_estimated_delivery_date\"))) \\\n","             .withColumn(\"order_status\", lower(col(\"order_status\"))) \\\n","             .withColumn(\"purchase_year\", year(col(\"order_purchase_timestamp\"))) \\\n","             .withColumn(\"purchase_month\", month(col(\"order_purchase_timestamp\"))) \\\n","             .na.drop(subset=[\"order_id\", \"customer_id\", \"order_purchase_timestamp\"])\n","\n","def transform_order_items(df: DataFrame) -> DataFrame:\n","    \"\"\"Cleans and transforms order items data.\"\"\"\n","    return df.withColumn(\"shipping_limit_date\", to_timestamp(col(\"shipping_limit_date\"))) \\\n","             .withColumn(\"price\", round(col(\"price\").cast(\"float\"), 2)) \\\n","             .withColumn(\"freight_value\", round(col(\"freight_value\").cast(\"float\"), 2)) \\\n","             .withColumn(\"order_item_id\", col(\"order_item_id\").cast(\"integer\"))\n","\n","def transform_payments(df: DataFrame) -> DataFrame:\n","    \"\"\"Cleans and transforms payments data.\"\"\"\n","    return df.withColumn(\"payment_sequential\", col(\"payment_sequential\").cast(\"integer\")) \\\n","             .withColumn(\"payment_installments\", col(\"payment_installments\").cast(\"integer\")) \\\n","             .withColumn(\"payment_value\", round(col(\"payment_value\").cast(\"float\"), 2))\n","\n","def transform_reviews(df: DataFrame) -> DataFrame:\n","    \"\"\"Cleans and transforms reviews data.\"\"\"\n","    return df.withColumn(\"review_creation_date\", to_timestamp(col(\"review_creation_date\"))) \\\n","             .withColumn(\"review_answer_timestamp\", to_timestamp(col(\"review_answer_timestamp\"))) \\\n","             .withColumn(\"review_score\", col(\"review_score\").cast(\"integer\")) \\\n","             .na.drop(subset=[\"order_id\", \"review_score\"])\n","\n","def transform_products(df: DataFrame) -> DataFrame:\n","    \"\"\"Cleans and transforms products data.\"\"\"\n","    return df.withColumnRenamed(\"product_name_lenght\", \"product_name_length\") \\\n","             .withColumnRenamed(\"product_description_lenght\", \"product_description_length\")\n","\n","def passthrough_transform(df: DataFrame) -> DataFrame:\n","    \"\"\"A default function for data that needs no transformation.\"\"\"\n","    return df\n","\n","# =============================================================================\n","# 2. DETECT AVAILABLE PATHS AND LAKEHOUSES\n","# =============================================================================\n","\n","print(\"üîç Detecting available paths and lakehouses...\")\n","\n","# Try to list available lakehouses/paths\n","try:\n","    # Check what's available in the root\n","    print(\"Root directories:\")\n","    dbutils.fs.ls(\"/\")\n","except:\n","    print(\"Cannot access root with dbutils.fs\")\n","\n","# Try different path patterns for Bronze layer\n","bronze_paths_to_try = [\n","    \"abfss://Alex_Olist@onelake.dfs.fabric.microsoft.com/LH_Bronze_AY.Lakehouse/Files/\",\n","    \"/LH_Bronze_AY.Lakehouse/Files/\",\n","    \"LH_Bronze_AY.Lakehouse/Files/\",\n","    \"Files/\",\n","    \"\"\n","]\n","\n","# Try different path patterns for Silver layer  \n","silver_paths_to_try = [\n","    \"abfss://Alex_Olist@onelake.dfs.fabric.microsoft.com/LH_Silver_AY.Lakehouse/Files/\",\n","    \"/LH_Silver_AY.Lakehouse/Files/\",\n","    \"LH_Silver_AY.Lakehouse/Files/\",\n","    \"../LH_Silver_AY.Lakehouse/Files/\",\n","    \"Files/\"\n","]\n","\n","# Test read path\n","bronze_base_path = None\n","for path in bronze_paths_to_try:\n","    try:\n","        test_file = f\"{path}olist_orders_dataset.csv\"\n","        print(f\"Testing read path: {test_file}\")\n","        test_df = spark.read.format(\"csv\").option(\"header\", \"true\").load(test_file)\n","        if test_df.count() > 0:\n","            bronze_base_path = path\n","            print(f\"‚úÖ Found working Bronze path: {bronze_base_path}\")\n","            break\n","    except Exception as e:\n","        print(f\"‚ùå Failed: {test_file} - {str(e)[:100]}\")\n","\n","if not bronze_base_path:\n","    print(\"‚ùå Could not find Bronze layer files. Please check the file paths.\")\n","    print(\"Available files in current directory:\")\n","    try:\n","        for file_info in dbutils.fs.ls(\".\"):\n","            print(f\"  {file_info.name}\")\n","    except:\n","        pass\n","    exit()\n","\n","# Test write path\n","silver_base_path = None\n","for path in silver_paths_to_try:\n","    try:\n","        # Try to create the directory structure\n","        test_write_path = f\"{path}test_folder/\"\n","        print(f\"Testing write path: {test_write_path}\")\n","        \n","        # Create a simple test dataframe\n","        test_data = spark.createDataFrame([(\"test\",)], [\"col1\"])\n","        test_data.coalesce(1).write.mode(\"overwrite\").format(\"csv\").option(\"header\", \"true\").save(test_write_path)\n","        \n","        # Clean up test\n","        try:\n","            dbutils.fs.rm(test_write_path, True)\n","        except:\n","            pass\n","            \n","        silver_base_path = path\n","        print(f\"‚úÖ Found working Silver path: {silver_base_path}\")\n","        break\n","    except Exception as e:\n","        print(f\"‚ùå Failed: {test_write_path} - {str(e)[:100]}\")\n","\n","if not silver_base_path:\n","    print(\"‚ùå Could not access Silver layer for writing. Using Bronze path as fallback.\")\n","    silver_base_path = bronze_base_path\n","\n","# =============================================================================\n","# 3. CREATE CONFIGURATION\n","# =============================================================================\n","\n","job_config = {\n","    \"orders\":       {\"bronze\": \"olist_orders_dataset.csv\", \"silver\": \"silver_orders\", \"func\": transform_orders},\n","    \"customers\":    {\"bronze\": \"olist_customers_dataset.csv\", \"silver\": \"silver_customers\", \"func\": passthrough_transform},\n","    \"order_items\":  {\"bronze\": \"olist_order_items_dataset.csv\", \"silver\": \"silver_order_items\", \"func\": transform_order_items},\n","    \"products\":     {\"bronze\": \"olist_products_dataset.csv\", \"silver\": \"silver_products\", \"func\": transform_products},\n","    \"sellers\":      {\"bronze\": \"olist_sellers_dataset.csv\", \"silver\": \"silver_sellers\", \"func\": passthrough_transform},\n","    \"payments\":     {\"bronze\": \"olist_order_payments_dataset.csv\", \"silver\": \"silver_order_payments\", \"func\": transform_payments},\n","    \"reviews\":      {\"bronze\": \"olist_order_reviews_dataset.csv\", \"silver\": \"silver_order_reviews\", \"func\": transform_reviews},\n","    \"geolocation\":  {\"bronze\": \"olist_geolocation_dataset.csv\", \"silver\": \"silver_geolocation\", \"func\": passthrough_transform},\n","    \"translations\": {\"bronze\": \"product_category_name_translation.csv\", \"silver\": \"silver_product_translations\", \"func\": passthrough_transform}\n","}\n","\n","# =============================================================================\n","# 4. RUN THE TRANSFORMATION JOB\n","# =============================================================================\n","\n","print(f\"\\nüöÄ Starting CSV transformation job...\")\n","print(f\"üìÅ Reading from: {bronze_base_path}\")\n","print(f\"üìÅ Writing to: {silver_base_path}\")\n","\n","successful_transformations = 0\n","\n","for key, config in job_config.items():\n","    bronze_filename = config[\"bronze\"]\n","    silver_filename = config[\"silver\"]\n","    transform_function = config[\"func\"]\n","    \n","    read_path = f\"{bronze_base_path}{bronze_filename}\"\n","    write_path = f\"{silver_base_path}{silver_filename}.csv\"\n","    \n","    try:\n","        print(f\"\\nüìä Processing '{key}': {bronze_filename} -> {silver_filename}.csv\")\n","        \n","        # Step 1: Read the CSV file\n","        bronze_df = spark.read \\\n","            .format(\"csv\") \\\n","            .option(\"header\", \"true\") \\\n","            .option(\"inferSchema\", \"true\") \\\n","            .option(\"multiline\", \"true\") \\\n","            .option(\"escape\", '\"') \\\n","            .load(read_path)\n","        \n","        row_count = bronze_df.count()\n","        if row_count == 0:\n","            print(f\"‚ö†Ô∏è  WARNING: '{key}' appears to be empty, skipping...\")\n","            continue\n","        \n","        print(f\"   üìà Loaded {row_count} rows\")\n","        \n","        # Step 2: Apply transformation\n","        print(f\"   üîÑ Applying transformation: {transform_function.__name__}\")\n","        transformed_df = transform_function(bronze_df)\n","        \n","        # Step 3: Clean column names\n","        final_df = transformed_df.toDF(*(c.lower().replace(\" \", \"_\") for c in transformed_df.columns))\n","        \n","        # Step 4: Write as single CSV file\n","        print(f\"   üíæ Writing to Silver layer...\")\n","        final_df.coalesce(1) \\\n","            .write \\\n","            .mode(\"overwrite\") \\\n","            .format(\"csv\") \\\n","            .option(\"header\", \"true\") \\\n","            .save(write_path)\n","        \n","        print(f\"   ‚úÖ Successfully processed '{key}' - {row_count} rows written to {silver_filename}.csv\")\n","        successful_transformations += 1\n","        \n","    except Exception as e:\n","        print(f\"   ‚ùå ERROR processing '{key}': {str(e)}\")\n","        print(f\"      Read path: {read_path}\")\n","        print(f\"      Write path: {write_path}\")\n","        continue\n","\n","print(f\"\\nüéâ Transformation job completed!\")\n","print(f\"üìà Successfully processed {successful_transformations}/{len(job_config)} files\")\n","\n","if successful_transformations > 0:\n","    print(f\"\\nüìã Transformed CSV files are now available in LH_Silver_AY:\")\n","    for key, config in job_config.items():\n","        print(f\"   ‚Ä¢ {config['silver']}.csv\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":16,"statement_ids":[16],"state":"finished","livy_statement_state":"available","session_id":"683d243f-7cc7-4b06-af7e-979352f7fb30","normalized_state":"finished","queued_time":"2025-09-14T07:40:41.9427576Z","session_start_time":null,"execution_start_time":"2025-09-14T07:40:41.9451639Z","execution_finish_time":"2025-09-14T07:41:26.8762004Z","parent_msg_id":"d90031f5-ebd4-4cb0-a757-cb9d4e588497"},"text/plain":"StatementMeta(, 683d243f-7cc7-4b06-af7e-979352f7fb30, 16, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["üîç Detecting available paths and lakehouses...\nRoot directories:\nCannot access root with dbutils.fs\nTesting read path: abfss://Alex_Olist@onelake.dfs.fabric.microsoft.com/LH_Bronze_AY.Lakehouse/Files/olist_orders_dataset.csv\n‚úÖ Found working Bronze path: abfss://Alex_Olist@onelake.dfs.fabric.microsoft.com/LH_Bronze_AY.Lakehouse/Files/\nTesting write path: abfss://Alex_Olist@onelake.dfs.fabric.microsoft.com/LH_Silver_AY.Lakehouse/Files/test_folder/\n‚úÖ Found working Silver path: abfss://Alex_Olist@onelake.dfs.fabric.microsoft.com/LH_Silver_AY.Lakehouse/Files/\n\nüöÄ Starting CSV transformation job...\nüìÅ Reading from: abfss://Alex_Olist@onelake.dfs.fabric.microsoft.com/LH_Bronze_AY.Lakehouse/Files/\nüìÅ Writing to: abfss://Alex_Olist@onelake.dfs.fabric.microsoft.com/LH_Silver_AY.Lakehouse/Files/\n\nüìä Processing 'orders': olist_orders_dataset.csv -> silver_orders.csv\n   üìà Loaded 99441 rows\n   üîÑ Applying transformation: transform_orders\n   üíæ Writing to Silver layer...\n   ‚úÖ Successfully processed 'orders' - 99441 rows written to silver_orders.csv\n\nüìä Processing 'customers': olist_customers_dataset.csv -> silver_customers.csv\n   üìà Loaded 99441 rows\n   üîÑ Applying transformation: passthrough_transform\n   üíæ Writing to Silver layer...\n   ‚úÖ Successfully processed 'customers' - 99441 rows written to silver_customers.csv\n\nüìä Processing 'order_items': olist_order_items_dataset.csv -> silver_order_items.csv\n   üìà Loaded 112650 rows\n   üîÑ Applying transformation: transform_order_items\n   üíæ Writing to Silver layer...\n   ‚úÖ Successfully processed 'order_items' - 112650 rows written to silver_order_items.csv\n\nüìä Processing 'products': olist_products_dataset.csv -> silver_products.csv\n   üìà Loaded 32951 rows\n   üîÑ Applying transformation: transform_products\n   üíæ Writing to Silver layer...\n   ‚úÖ Successfully processed 'products' - 32951 rows written to silver_products.csv\n\nüìä Processing 'sellers': olist_sellers_dataset.csv -> silver_sellers.csv\n   üìà Loaded 3095 rows\n   üîÑ Applying transformation: passthrough_transform\n   üíæ Writing to Silver layer...\n   ‚úÖ Successfully processed 'sellers' - 3095 rows written to silver_sellers.csv\n\nüìä Processing 'payments': olist_order_payments_dataset.csv -> silver_order_payments.csv\n   üìà Loaded 103886 rows\n   üîÑ Applying transformation: transform_payments\n   üíæ Writing to Silver layer...\n   ‚úÖ Successfully processed 'payments' - 103886 rows written to silver_order_payments.csv\n\nüìä Processing 'reviews': olist_order_reviews_dataset.csv -> silver_order_reviews.csv\n   üìà Loaded 99224 rows\n   üîÑ Applying transformation: transform_reviews\n   üíæ Writing to Silver layer...\n   ‚úÖ Successfully processed 'reviews' - 99224 rows written to silver_order_reviews.csv\n\nüìä Processing 'geolocation': olist_geolocation_dataset.csv -> silver_geolocation.csv\n   üìà Loaded 1000163 rows\n   üîÑ Applying transformation: passthrough_transform\n   üíæ Writing to Silver layer...\n   ‚úÖ Successfully processed 'geolocation' - 1000163 rows written to silver_geolocation.csv\n\nüìä Processing 'translations': product_category_name_translation.csv -> silver_product_translations.csv\n   üìà Loaded 71 rows\n   üîÑ Applying transformation: passthrough_transform\n   üíæ Writing to Silver layer...\n   ‚úÖ Successfully processed 'translations' - 71 rows written to silver_product_translations.csv\n\nüéâ Transformation job completed!\nüìà Successfully processed 9/9 files\n\nüìã Transformed CSV files are now available in LH_Silver_AY:\n   ‚Ä¢ silver_orders.csv\n   ‚Ä¢ silver_customers.csv\n   ‚Ä¢ silver_order_items.csv\n   ‚Ä¢ silver_products.csv\n   ‚Ä¢ silver_sellers.csv\n   ‚Ä¢ silver_order_payments.csv\n   ‚Ä¢ silver_order_reviews.csv\n   ‚Ä¢ silver_geolocation.csv\n   ‚Ä¢ silver_product_translations.csv\n"]}],"execution_count":14,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"8bdae258-f4b2-4c21-b61d-da6171868cfe"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","display_name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"known_lakehouses":[{"id":"5c636dcf-06b5-430a-99cb-74a9c3bb6eaa"},{"id":"5761e729-e6b4-44af-a20c-528a3daa8de9"}],"default_lakehouse":"5c636dcf-06b5-430a-99cb-74a9c3bb6eaa","default_lakehouse_name":"LH_Bronze_AY","default_lakehouse_workspace_id":"6ada6014-a716-46d4-bcf0-9a41c370a40f"}}},"nbformat":4,"nbformat_minor":5}